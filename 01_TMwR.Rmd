---
title: "Tidy Modeling with R"
author: "Jeffrey Long"
date: "1/5/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Hello World

This is my workbook for the book 
["Tidy Modeling with R"](https://www.tmwr.org/).
It uses a new collection of software in the R programming 
language for model building. 

* First and foremost, this book provides an introduction 
to how to use our software to create models. We focus on a
dialect of R called the 
[tidyverse](https://www.tidyverse.org/) that is designed 
to be a better interface for common tasks using R. If 
you’ve never heard of or used the tidyverse, Chapter 2 
provides an introduction. In this book, we demonstrate how 
the tidyverse can be used to produce high quality models. 
The tools used to do this are referred to as the 
_tidymodels packages_.

* Second, we use the tidymodels packages to 
**encourage good methodology and statistical practice**. 
Many models, especially complex predictive or machine 
learning models, can work very well on the data at hand 
but may fail when exposed to new data. Often, this issue 
is due to poor choices made during the development and/or 
selection of the models. Whenever possible, our software, 
documentation, and other materials attempt to prevent 
these and other pitfalls.

## Chapter 1: Software for modeling

Models are mathematical tools that can describe a system 
and capture relationships in the data given to them. 
Models can be used for various purposes, including 
predicting future events, determining if there is a 
difference between several groups, aiding map-based 
visualization, discovering novel patterns in the data that 
could be further investigated, and more. The utility of a 
model hinges on its ability to be reductive. The primary 
influences in the data can be captured mathematically in a 
useful way, such as in a relationship that can be 
expressed as an equation.

Since the beginning of the twenty-first century, 
mathematical models have become ubiquitous in our daily 
lives, in both obvious and subtle ways. A typical day for 
many people might involve checking the weather to see when 
might be a good time to walk the dog, ordering a product 
from a website, typing a text message to a friend and 
having it autocorrected, and checking email. In each of 
these instances, there is a good chance that some type of 
model was involved. In some cases, the contribution of the 
model might be easily perceived (“You might also be 
interested in purchasing product X”) while in other cases, 
the impact could be the absence of something (e.g., spam 
email). Models are used to choose clothing that a customer 
might like, to identify a molecule that should be 
evaluated as a drug candidate, and might even be the 
mechanism that a nefarious company uses to avoid the 
discovery of cars that over-pollute. For better or worse, 
models are here to stay.

|     There are two reasons that models permeate our 
|     lives today: an abundance of software exists to 
|     create models and it has become easier to record 
|     data and make it accessible.

This book focuses largely on software. It is obviously 
critical that software produces the correct relationships 
to represent the data. For the most part, determining 
mathematical correctness is possible, but the reliable 
creation of appropriate models requires more.

First, it is important that it is easy to operate software 
in a proper way. The user interface should not be so 
poorly designed that the user would not know that they 
used it inappropriately. For example, 
Baggerly and Coombes 
(2009) report myriad problems in the data analyses from a 
high profile computational biology publication. One of the 
issues was related to how the users were required to add 
the names of the model inputs. The user interface of the 
software made it easy to offset the column names of the 
data from the actual data columns. This resulted in the 
wrong genes being identified as important for treating 
cancer patients and eventually contributed to the 
termination of several clinical trials 
(Carlson 2012).

If we need high quality models, software must facilitate proper usage. Abrams (2003) describes an interesting principle to guide us:

|   The Pit of Success: in stark contrast to a summit, a 
|   peak, or a journey across a desert to find victory 
|   through many trials and surprises, we want our 
|   customers to simply fall into winning practices by 
|   using our platform and frameworks.

Data analysis and modeling software should espouse this 
idea.

The second important aspect of model building is related 
to scientific methodology. When working with complex 
predictive models, it can be easy to unknowingly commit 
errors related to logical fallacies or inappropriate 
assumptions. Many machine learning models are so adept at 
discovering patterns that they can effortlessly find 
empirical patterns in the data that fail to reproduce 
later. Some of these types of methodological errors are 
insidious in that the issue can go undetected until a 
later time when new data that contain the true result are 
obtained.

|   As our models have become more powerful and complex, |   it has also become easier to commit latent errors.

This same principle also applies to programming. Whenever 
possible, the software should be able to protect users 
from committing mistakes. Software should make it easy 
for users to **do the right thing**.

These two aspects of model development are crucial. Since tools for creating models are easily obtained and models can have such a profound impact, many more people are creating them. In terms of technical expertise and training, their backgrounds will vary. It is important that their tools be robust to the experience of the user. Tools should be powerful enough to create high-performance models, but, on the other hand, should be easy to use in an appropriate way. This book describes a suite of software for modeling which has been designed with these characteristics in mind.

The software is based on the R programming language (R 
Core Team 2014). R has been designed especially for data 
analysis and modeling. It is an implementation of the S 
language (with lexical scoping rules adapted from Scheme 
and Lisp) which was created in the 1970s to
“turn ideas into software, quickly and faithfully” 
(Chambers 1998).

R is open-source and free of charge. It is a powerful 
programming language that can be used for many different 
purposes but specializes in data analysis, modeling, 
visualization, and machine learning. R is easily 
extensible; it has a vast ecosystem of packages, mostly 
user-contributed modules that focus on a specific theme, 
such as modeling, visualization, and so on.

One collection of packages is called the tidyverse 
(Wickham et al. 2019). The tidyverse is an opinionated 
collection of R packages designed for data science. All 
packages share an underlying design philosophy, grammar, 
and data structures. Several of these design philosophies 
are directly informed by the aspects of software 
described in this section. If you’ve never used the 
tidyverse packages, Chapter 2 contains a review of its 
basic concepts. Within the tidyverse, the subset of 
packages specifically focused on modeling are referred to 
as the tidymodels packages. This book is an extended 
software manual for conducting modeling using the 
tidyverse and tidymodels. It shows how to use a set of 
packages, each with its own specific purpose, together to 
create high-quality models.

### Types of Models

Let’s describe a taxonomy for types of models, grouped by 
purpose. While not exhaustive, most models fall into at 
least one of these categories:

#### Descriptive Models







## REFERENCES

Abrams, B. 2003. “The Pit of Success.” 
https://blogs.msdn.microsoft.com/brada/2003/10/02/the-pit-of-success/.

Baggerly, K, and K Coombes. 2009. “Deriving 
Chemosensitivity from Cell Lines: Forensic Bioinformatics 
and Reproducible Research in High-Throughput Biology.” 
The Annals of Applied Statistics 3 (4): 1309–34.

Bolstad, B. 2004. Low-Level Analysis of High-Density 
Oligonucleotide Array Data: Background, Normalization and 
Summarization. University of California, Berkeley.

Breiman, L. 2001b. “Statistical Modeling: The Two 
Cultures.” Statistical Science 16 (3): 199–231.

Carlson, B. 2012. “Putting Oncology Patients at Risk.” 
Biotechnology Healthcare 9 (3): 17–21.

Chambers, J. 1998. Programming with Data: A Guide to the 
S Language. Berlin, Heidelberg: Springer-Verlag.

Cleveland, W. 1979. “Robust Locally Weighted Regression 
and Smoothing Scatterplots.” Journal of the American 
Statistical Association 74 (368): 829–36.

Durrleman, S, and R Simon. 1989. “Flexible Regression 
Models with Cubic Splines.” Statistics in Medicine 8 (5): 
551–61.

Gentleman, R, V Carey, W Huber, R Irizarry, and S Dudoit. 
2005. Bioinformatics and Computational Biology Solutions 
Using R and Bioconductor. Berlin, Heidelberg: 
Springer-Verlag.

Kuhn, M, and K Johnson. 2020. Feature Engineering and 
Selection: A Practical Approach for Predictive Models. 
CRC Press.

R Core Team. 2014. R: A Language and Environment for 
Statistical Computing. Vienna, Austria: R Foundation for 
Statistical Computing. http://www.R-project.org/.

Shmueli, G. 2010. “To Explain or to Predict?” Statistical 
Science 25 (3): 289–310.

Wickham, H, M Averick, J Bryan, W Chang, L McGowan, R 
François, G Grolemund, et al. 2019. “Welcome to the 
Tidyverse.” Journal of Open Source Software 4 (43).

Wickham, H, and G Grolemund. 2016. R for Data Science: 
Import, Tidy, Transform, Visualize, and Model Data. 
O’Reilly Media, Inc.
